# -*- coding: utf-8 -*-
"""BT4221 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iHe2y9kIYJcmyZJ_pbmHwyfFg98EAiI0
"""

# Import Modules and Files
from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.insert(0,"/content/drive/MyDrive/BT4221Project")
from src.preprocessing import train_val_test_split, parse_input_file, transform_df
from src.metrics import one_hot_encode_labels, plot_metrics, compute_score
from src.utility import find_largest_resolution, unzip

from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, CategoricalAccuracy, Precision, Recall, AUC
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation, SpatialDropout2D, BatchNormalization
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import History, EarlyStopping, ReduceLROnPlateau
from typing import Tuple
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import seaborn as sns

!unzip drive/MyDrive/BT4221Project/img.zip

# Import files
train_df = pd.read_csv('/content/train_df.csv').astype({'class': str})
val_df = pd.read_csv('/content/val_df.csv').astype({'class': str})
test_df = pd.read_csv('/content/test_df.csv').astype({'class': str})

num_categories = len(train_df['class'].unique())
num_categories

# Data Preparation
train_datagen = ImageDataGenerator(
    rescale=1./255
)
val_datagen = ImageDataGenerator(
    rescale=1./255
)
test_datagen = ImageDataGenerator(
    rescale=1./255
)

train_gen = train_datagen.flow_from_dataframe(
    train_df,
    x_col="filename", y_col="class",
    weight_col=None, target_size=(150, 150), color_mode='rgb',
    batch_size=32,
    class_mode='categorical', 
    shuffle=True
)

val_gen = val_datagen.flow_from_dataframe(
    val_df,
    x_col="filename", y_col="class",
    weight_col=None, target_size=(150, 150), color_mode='rgb',
    batch_size=32,
    class_mode='categorical', 
    shuffle=False
)

test_gen = test_datagen.flow_from_dataframe(
    test_df,
    x_col="filename", y_col="class",
    weight_col=None, target_size=(150, 150), color_mode='rgb',
    batch_size=32,
    class_mode='categorical', 
    shuffle=False
)

plt.figure(figsize=(16, 10))
for image_batch, labels_batch in train_gen:
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(image_batch[i])
        plt.title(labels_batch[i])
        plt.axis("off")        
    break

def base_model() -> Sequential:
    model = Sequential()
    model.add(Input((150, 150, 3)))

    # Conv2D Layer  
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    #model.add(SpatialDropout2D(0.2))

    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    

    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))

    model.add(Flatten())

    model.add(Dense(32, activation='relu'))
    #model.add(Dropout(0.2))

    model.add(Dense(3, activation='softmax'))
    
    return model

# Double the number of layers and number of neurons
def double_model() -> Sequential:
    model = Sequential()
    model.add(Input((150, 150, 3)))

     # Conv2D Layer  
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    #model.add(SpatialDropout2D(0.2))

    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    

    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))

    # Conv2D Layer
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    #model.add(SpatialDropout2D(0.2))

    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    

    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))

    model.add(Flatten())

    # Doubled the number of neurons
    model.add(Dense(64, activation='relu'))
    #model.add(Dropout(0.2))

    model.add(Dense(3, activation='softmax'))
    
    return model

# Triple the number of layers and number of neurons
def triple_model() -> Sequential:
    model = Sequential()
    model.add(Input((150, 150, 3)))

    # Conv2D Layer  
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    #model.add(SpatialDropout2D(0.2))

    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    

    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))

    # Conv2D Layer
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    #model.add(SpatialDropout2D(0.2))

    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    

    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))

    # Conv2D Layer
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    #model.add(SpatialDropout2D(0.2))

    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    

    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))

    model.add(Flatten())

    # Triple the number of neurons
    model.add(Dense(128, activation='relu'))
    #model.add(Dropout(0.2))

    model.add(Dense(3, activation='softmax'))
    
    return model

METRICS = [
      CategoricalAccuracy(name='accuracy'),
      Precision(name='precision'),
      Recall(name='recall'),
      AUC(name='auc'),
      AUC(name='prc', curve='PR')
]

number_epochs = 20
early_stoppage = EarlyStopping(monitor='val_loss', patience=3)
variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2, min_lr=0.0001)

# Base model with one layer
model = base_model()
model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=METRICS)
model.summary()

history = model.fit(
    train_gen,
    epochs=number_epochs,
    validation_data=val_gen,
    callbacks=[variable_learning_rate, early_stoppage]    
)

plot_metrics(history)

y_test = one_hot_encode_labels(test_gen.labels)
y_pred = model.predict(test_gen)
compute_score("Test Dataset", y_test, y_pred)

# Model with double the number of layers and neurons
model2 = double_model()
model2.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=METRICS)
model2.summary()

history2 = model2.fit(
    train_gen,
    epochs=number_epochs,
    validation_data=val_gen,
    callbacks=[variable_learning_rate, early_stoppage]    
)

plot_metrics(history2)

y_test = one_hot_encode_labels(test_gen.labels)
y_pred = model2.predict(test_gen)
compute_score("Test Dataset", y_test, y_pred)

# Model with triple the number of layers and neurons
model3 = triple_model()
model3.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=METRICS)
model3.summary()

history3 = model3.fit(
    train_gen,
    epochs=number_epochs,
    validation_data=val_gen,
    callbacks=[variable_learning_rate, early_stoppage]    
)

plot_metrics(history3)

y_test = one_hot_encode_labels(test_gen.labels)
y_pred = model3.predict(test_gen)
compute_score("Test Dataset", y_test, y_pred)