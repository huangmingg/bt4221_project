# -*- coding: utf-8 -*-
"""BT4221 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iHe2y9kIYJcmyZJ_pbmHwyfFg98EAiI0
"""

# Import Modules and Files
from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.insert(0,"/content/drive/MyDrive/BT4221Project")
from src.preprocessing import train_val_test_split, parse_input_file, transform_df
from src.metrics import one_hot_encode_labels, plot_metrics, compute_score
from src.utility import find_largest_resolution, unzip

from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, CategoricalAccuracy, Precision, Recall, AUC
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation, SpatialDropout2D, BatchNormalization
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import History, EarlyStopping, ReduceLROnPlateau
from typing import Tuple
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import seaborn as sns


# directory to image folder - change this accordingly
DTIF = os.path.join('data', 'img')
CAT_LABEL_FILEPATH = 'category_label.csv'
DF_FILEPATH = 'list_category_img.txt'


# experimenting different target size
df = parse_input_file(DF_FILEPATH, DTIF, CAT_LABEL_FILEPATH)
df = transform_df(df)
train_df, val_df, test_df = train_val_test_split(df)
num_categories = len(train_df['class'].unique())
target_size_arr = [48, 75, 150, 301]

def base_model(input_wh: Tuple[int, int]) -> Sequential:
    model = Sequential()
    model.add(Input((input_wh[0], input_wh[1], 3)))
    # Conv2D Layer  
    model.add(Conv2D(filters=64, kernel_size=3, padding='same'))
    model.add(Activation('relu'))    
    # Batch Normalization
    model.add(BatchNormalization())
    model.add(Activation('relu'))    
    # Max Pooling
    model.add(MaxPooling2D(pool_size=(4, 4), strides = 3))
    model.add(Flatten())
    model.add(Dense(32, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    return model

METRICS = [
      CategoricalAccuracy(name='accuracy'),
      Precision(name='precision'),
      Recall(name='recall'),
      AUC(name='auc'),
      AUC(name='prc', curve='PR')
]

number_epochs = 10
history_arr = []

# Model Performance for each target size
for i in target_size_arr:
    model = base_model((i, i))
    model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=METRICS)
    model.summary()
    history = model.fit(
        ImageDataGenerator(rescale=1./255).
        flow_from_dataframe(
            train_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(i, i), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        ),
        epochs=number_epochs,
        validation_data=ImageDataGenerator(rescale=1./255).
        flow_from_dataframe(
            val_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(i, i), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        )
    )

    plot_metrics(history)
    history_arr.append(history)
    test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
            test_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(i, i), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        )
    y_test = one_hot_encode_labels(test_gen.labels)
    y_pred = model.predict(test_gen)
    compute_score("Test Dataset", y_test, y_pred)

print(history_arr)


# experimenting sampling strategy
history_arr = []
strategies_arr = [None, 'under', 'over']
for i in strategies_arr:
    df = parse_input_file(DF_FILEPATH, DTIF, CAT_LABEL_FILEPATH)
    df = transform_df(df, sampling_strategy=i)
    train_df, val_df, test_df = train_val_test_split(df)
    num_categories = len(train_df['class'].unique())
    
    model = base_model((150, 150))
    model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=METRICS)
    model.summary()
    history = model.fit(
        ImageDataGenerator(rescale=1./255).
        flow_from_dataframe(
            train_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(150, 150), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        ),
        epochs=number_epochs,
        validation_data=ImageDataGenerator(rescale=1./255).
        flow_from_dataframe(
            val_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(150, 150), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        )
    )

    plot_metrics(history)
    history_arr.append(history)
    test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
            test_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(150, 150), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        )
    y_test = one_hot_encode_labels(test_gen.labels)
    y_pred = model.predict(test_gen)
    compute_score("Test Dataset", y_test, y_pred)

print(history_arr)


# experimenting data augmentation
history_arr = []
augmentation_arr = [
    ("rotation", ImageDataGenerator(rescale=1./255, rotation_range=45)),
    ("flips", ImageDataGenerator(rescale=1./255, vertical_flip=True, horizontal_flip=True)),
    ("shear", ImageDataGenerator(rescale=1./255, shear_range=45.0)),
    ("zoom", ImageDataGenerator(rescale=1./255, zoom_range=0.3)),

]

df = parse_input_file(DF_FILEPATH, DTIF, CAT_LABEL_FILEPATH)
df = transform_df(df, sampling_strategy='under')
train_df, val_df, test_df = train_val_test_split(df)
num_categories = len(train_df['class'].unique())    
model = base_model((150, 150))
model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=METRICS)
model.summary()
for i in augmentation_arr:
    print(f"Model for data augmentation {i[0]}")
    history = model.fit(
            i[1].
            flow_from_dataframe(
                train_df,
                x_col="filename", 
                y_col="class",
                weight_col=None, 
                target_size=(150, 150), 
                color_mode='rgb',
                batch_size=32,
                class_mode='categorical', 
                shuffle=True
            ),
            epochs=number_epochs,
            validation_data=i[1].
            flow_from_dataframe(
                val_df,
                x_col="filename", 
                y_col="class",
                weight_col=None, 
                target_size=(150, 150), 
                color_mode='rgb',
                batch_size=32,
                class_mode='categorical', 
                shuffle=True
            )
        )
    plot_metrics(history)
    history_arr.append(history)
    test_gen = i[1].flow_from_dataframe(
            test_df,
            x_col="filename", 
            y_col="class",
            weight_col=None, 
            target_size=(150, 150), 
            color_mode='rgb',
            batch_size=32,
            class_mode='categorical', 
            shuffle=True
        )
    y_test = one_hot_encode_labels(test_gen.labels)
    y_pred = model.predict(test_gen)
    compute_score("Test Dataset", y_test, y_pred)        
    
print(history_arr)